import json

# Create improved notebook with batch management
notebook = {
    "cells": [
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": ["%pip install openai python-dotenv"]
        },
        {
            "cell_type": "code", 
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "from openai import OpenAI\n",
                "import os\n",
                "import json\n",
                "import time\n",
                "from dotenv import load_dotenv\n",
                "\n",
                "load_dotenv()\n",
                "client = OpenAI()\n",
                "\n",
                "test_path = os.path.abspath(os.getcwd()).split('gpt5_completion_scripts')[0] + '/processed_data/gpt5/processed_test.jsonl'\n",
                "output_dir = os.path.abspath(os.getcwd()).split('gpt5_completion_scripts')[0] + '/generated_data_gpt5'\n",
                "os.makedirs(output_dir, exist_ok=True)\n",
                "\n",
                "test_data = []\n",
                "with open(test_path, 'r') as f:\n",
                "    for line in f:\n",
                "        test_data.append(json.loads(line))\n",
                "\n",
                "print(f\"Loaded {len(test_data)} test items\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Check current batch status\n",
                "def check_batch_status():\n",
                "    batches = client.batches.list(limit=20)\n",
                "    print(\"Current batches:\")\n",
                "    for batch in batches.data:\n",
                "        print(f\"ID: {batch.id}, Status: {batch.status}, Model: {getattr(batch, 'metadata', {}).get('model', 'unknown')}\")\n",
                "    return batches.data\n",
                "\n",
                "current_batches = check_batch_status()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": None, 
            "metadata": {},
            "outputs": [],
            "source": [
                "def load_prompt(filename):\n",
                "    with open(filename, 'r') as f:\n",
                "        return f.read()\n",
                "\n",
                "generator_system = load_prompt('generator_system_prompt.md')\n",
                "evaluator_system = load_prompt('evaluator_system_prompt.md')\n",
                "\n",
                "def extract_passage(prompt_text):\n",
                "    return prompt_text.split('###')[0].strip()\n",
                "\n",
                "def create_k_shot(data, k=3):\n",
                "    examples = []\n",
                "    for i in range(min(k, len(data))):\n",
                "        passage = extract_passage(data[i]['prompt'])\n",
                "        completion = data[i]['completion'].strip()\n",
                "        examples.append(f\"Passage: {passage}\\n\\nOutput: {completion}\")\n",
                "    return \"\\n\\n---\\n\\n\".join(examples)\n",
                "\n",
                "def wait_for_batch(batch_id, description=\"batch\"):\n",
                "    print(f\"Waiting for {description} to complete...\")\n",
                "    while True:\n",
                "        batch = client.batches.retrieve(batch_id)\n",
                "        print(f\"Status: {batch.status}\")\n",
                "        if batch.status == \"completed\":\n",
                "            return batch\n",
                "        elif batch.status == \"failed\":\n",
                "            print(f\"Batch failed: {batch}\")\n",
                "            return None\n",
                "        time.sleep(30)\n",
                "\n",
                "k_shot_examples = create_k_shot(test_data)\n",
                "print(\"Setup complete\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "# A1: Single-agent, standard model - SMALLER BATCH (100 items to avoid queue limit)\n",
                "def run_a1_small():\n",
                "    print(\"Running A1: Single-agent, standard model (100 items - reduced to avoid queue limit)\")\n",
                "    \n",
                "    # Create smaller batch to avoid queue limit\n",
                "    requests = []\n",
                "    for i in range(min(100, len(test_data))):\n",
                "        passage = extract_passage(test_data[i]['prompt'])\n",
                "        user_prompt = f\"Here are some examples:\\n\\n{k_shot_examples}\\n\\n---\\n\\nGenerate a quiz for this passage:\\n\\n{passage}\"\n",
                "        \n",
                "        requests.append({\n",
                "            \"custom_id\": f\"A1_{i+1}\",\n",
                "            \"method\": \"POST\",\n",
                "            \"url\": \"/v1/chat/completions\",\n",
                "            \"body\": {\n",
                "                \"model\": \"gpt-4\",\n",
                "                \"messages\": [\n",
                "                    {\"role\": \"system\", \"content\": generator_system},\n",
                "                    {\"role\": \"user\", \"content\": user_prompt}\n",
                "                ],\n",
                "                \"temperature\": 0.7,\n",
                "                \"max_tokens\": 200\n",
                "            }\n",
                "        })\n",
                "    \n",
                "    # Save batch file\n",
                "    batch_file = f\"{output_dir}/batch_a1_small.jsonl\"\n",
                "    with open(batch_file, 'w') as f:\n",
                "        for request in requests:\n",
                "            f.write(json.dumps(request) + '\\n')\n",
                "    \n",
                "    print(f\"Created batch file with {len(requests)} requests\")\n",
                "    print(f\"Estimated tokens: {len(requests) * 200} (should be under queue limit)\")\n",
                "    \n",
                "    # Upload and create batch\n",
                "    try:\n",
                "        with open(batch_file, 'rb') as f:\n",
                "            file_response = client.files.create(file=f, purpose=\"batch\")\n",
                "        \n",
                "        batch = client.batches.create(\n",
                "            input_file_id=file_response.id,\n",
                "            endpoint=\"/v1/chat/completions\",\n",
                "            completion_window=\"24h\",\n",
                "            metadata={\"scenario\": \"A1\", \"model\": \"gpt-4\"}\n",
                "        )\n",
                "        \n",
                "        print(f\"Batch created successfully: {batch.id}\")\n",
                "        return batch.id\n",
                "        \n",
                "    except Exception as e:\n",
                "        print(f\"Error creating batch: {e}\")\n",
                "        return None\n",
                "\n",
                "# Uncomment to run (but check queue first!)\n",
                "# a1_batch_id = run_a1_small()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "# A2: Single-agent, reasoning model (50 items - even smaller for GPT-5)\n",
                "def run_a2_small():\n",
                "    print(\"Running A2: Single-agent, reasoning model (50 items)\")\n",
                "    \n",
                "    # Create smaller batch for GPT-5\n",
                "    requests = []\n",
                "    for i in range(min(50, len(test_data))):\n",
                "        passage = extract_passage(test_data[i]['prompt'])\n",
                "        user_prompt = f\"Here are some examples:\\n\\n{k_shot_examples}\\n\\n---\\n\\nGenerate a quiz for this passage:\\n\\n{passage}\"\n",
                "        \n",
                "        requests.append({\n",
                "            \"custom_id\": f\"A2_{i+1}\",\n",
                "            \"method\": \"POST\",\n",
                "            \"url\": \"/v1/chat/completions\",\n",
                "            \"body\": {\n",
                "                \"model\": \"gpt-5-2025-08-07\",\n",
                "                \"messages\": [\n",
                "                    {\"role\": \"user\", \"content\": f\"{generator_system}\\n\\n{user_prompt}\"}\n",
                "                ]\n",
                "            }\n",
                "        })\n",
                "    \n",
                "    # Save batch file\n",
                "    batch_file = f\"{output_dir}/batch_a2_small.jsonl\"\n",
                "    with open(batch_file, 'w') as f:\n",
                "        for request in requests:\n",
                "            f.write(json.dumps(request) + '\\n')\n",
                "    \n",
                "    print(f\"Created batch file with {len(requests)} requests for GPT-5\")\n",
                "    \n",
                "    # Upload and create batch\n",
                "    try:\n",
                "        with open(batch_file, 'rb') as f:\n",
                "            file_response = client.files.create(file=f, purpose=\"batch\")\n",
                "        \n",
                "        batch = client.batches.create(\n",
                "            input_file_id=file_response.id,\n",
                "            endpoint=\"/v1/chat/completions\",\n",
                "            completion_window=\"24h\",\n",
                "            metadata={\"scenario\": \"A2\", \"model\": \"gpt-5-2025-08-07\"}\n",
                "        )\n",
                "        \n",
                "        print(f\"Batch created successfully: {batch.id}\")\n",
                "        return batch.id\n",
                "        \n",
                "    except Exception as e:\n",
                "        print(f\"Error creating batch: {e}\")\n",
                "        print(\"Note: If GPT-5 model name is incorrect, this will fail\")\n",
                "        return None\n",
                "\n",
                "# Uncomment to run\n",
                "# a2_batch_id = run_a2_small()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Helper function to retrieve and process completed batches\n",
                "def process_completed_batch(batch_id, scenario_name, output_filename):\n",
                "    try:\n",
                "        batch = client.batches.retrieve(batch_id)\n",
                "        \n",
                "        if batch.status != \"completed\":\n",
                "            print(f\"Batch {batch_id} status: {batch.status}\")\n",
                "            return None\n",
                "        \n",
                "        # Download results\n",
                "        result_file_id = batch.output_file_id\n",
                "        result = client.files.content(result_file_id)\n",
                "        \n",
                "        # Process results\n",
                "        results = {}\n",
                "        for line in result.text.strip().split('\\n'):\n",
                "            response = json.loads(line)\n",
                "            custom_id = response['custom_id']\n",
                "            \n",
                "            if 'error' in response:\n",
                "                print(f\"Error in {custom_id}: {response['error']}\")\n",
                "                continue\n",
                "                \n",
                "            quiz_content = response['response']['body']['choices'][0]['message']['content']\n",
                "            \n",
                "            results[custom_id] = {\n",
                "                \"item_id\": custom_id,\n",
                "                \"variant\": scenario_name,\n",
                "                \"round\": 1,\n",
                "                \"quiz\": quiz_content\n",
                "            }\n",
                "        \n",
                "        # Save final results\n",
                "        with open(f\"{output_dir}/{output_filename}\", 'w') as f:\n",
                "            json.dump(results, f, indent=2)\n",
                "        \n",
                "        print(f\"{scenario_name} completed: {len(results)} items saved to {output_filename}\")\n",
                "        return results\n",
                "        \n",
                "    except Exception as e:\n",
                "        print(f\"Error processing batch {batch_id}: {e}\")\n",
                "        return None\n",
                "\n",
                "# Example usage:\n",
                "# results_a1 = process_completed_batch(\"batch_id_here\", \"A1\", \"generated_quiz_A1.json\")\n",
                "# results_a2 = process_completed_batch(\"batch_id_here\", \"A2\", \"generated_quiz_A2.json\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"Batch management notebook ready!\")\n",
                "print(\"\")\n",
                "print(\"IMPORTANT STEPS:\")\n",
                "print(\"1. Run check_batch_status() to see current queue\")\n",
                "print(\"2. Wait for existing batches to complete if queue is full\")\n",
                "print(\"3. Start with smaller batches (100 items) to test\")\n",
                "print(\"4. Use process_completed_batch() to retrieve results\")\n",
                "print(\"\")\n",
                "print(\"Queue limit error means you have 100k+ tokens already queued.\")\n",
                "print(\"Wait for completion or cancel some batches before submitting new ones.\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python", 
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}

with open('gpt5_completions.ipynb', 'w') as f:
    json.dump(notebook, f, indent=1)

print("Fixed batch notebook created!")
