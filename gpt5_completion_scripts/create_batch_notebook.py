import json

# Create notebook with Batch API implementation
notebook = {
    "cells": [
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": ["%pip install openai python-dotenv"]
        },
        {
            "cell_type": "code", 
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "from openai import OpenAI\n",
                "import os\n",
                "import json\n",
                "import time\n",
                "from dotenv import load_dotenv\n",
                "\n",
                "load_dotenv()\n",
                "client = OpenAI()\n",
                "\n",
                "test_path = os.path.abspath(os.getcwd()).split('gpt5_completion_scripts')[0] + '/processed_data/gpt5/processed_test.jsonl'\n",
                "output_dir = os.path.abspath(os.getcwd()).split('gpt5_completion_scripts')[0] + '/generated_data_gpt5'\n",
                "os.makedirs(output_dir, exist_ok=True)\n",
                "\n",
                "test_data = []\n",
                "with open(test_path, 'r') as f:\n",
                "    for line in f:\n",
                "        test_data.append(json.loads(line))\n",
                "\n",
                "print(f\"Loaded {len(test_data)} test items\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": None, 
            "metadata": {},
            "outputs": [],
            "source": [
                "def load_prompt(filename):\n",
                "    with open(filename, 'r') as f:\n",
                "        return f.read()\n",
                "\n",
                "generator_system = load_prompt('generator_system_prompt.md')\n",
                "evaluator_system = load_prompt('evaluator_system_prompt.md')\n",
                "\n",
                "def extract_passage(prompt_text):\n",
                "    return prompt_text.split('###')[0].strip()\n",
                "\n",
                "def create_k_shot(data, k=3):\n",
                "    examples = []\n",
                "    for i in range(min(k, len(data))):\n",
                "        passage = extract_passage(data[i]['prompt'])\n",
                "        completion = data[i]['completion'].strip()\n",
                "        examples.append(f\"Passage: {passage}\\n\\nOutput: {completion}\")\n",
                "    return \"\\n\\n---\\n\\n\".join(examples)\n",
                "\n",
                "def wait_for_batch(batch_id, description=\"batch\"):\n",
                "    print(f\"Waiting for {description} to complete...\")\n",
                "    while True:\n",
                "        batch = client.batches.retrieve(batch_id)\n",
                "        print(f\"Status: {batch.status}\")\n",
                "        if batch.status == \"completed\":\n",
                "            return batch\n",
                "        elif batch.status == \"failed\":\n",
                "            print(f\"Batch failed: {batch}\")\n",
                "            return None\n",
                "        time.sleep(30)\n",
                "\n",
                "k_shot_examples = create_k_shot(test_data)\n",
                "print(\"Setup complete\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "# A1: Single-agent, standard model (950 items)\n",
                "def run_a1():\n",
                "    print(\"Running A1: Single-agent, standard model (950 items)\")\n",
                "    \n",
                "    # Create batch requests\n",
                "    requests = []\n",
                "    for i in range(min(950, len(test_data))):\n",
                "        passage = extract_passage(test_data[i]['prompt'])\n",
                "        user_prompt = f\"Here are some examples:\\n\\n{k_shot_examples}\\n\\n---\\n\\nGenerate a quiz for this passage:\\n\\n{passage}\"\n",
                "        \n",
                "        requests.append({\n",
                "            \"custom_id\": f\"A1_{i+1}\",\n",
                "            \"method\": \"POST\",\n",
                "            \"url\": \"/v1/chat/completions\",\n",
                "            \"body\": {\n",
                "                \"model\": \"gpt-4\",\n",
                "                \"messages\": [\n",
                "                    {\"role\": \"system\", \"content\": generator_system},\n",
                "                    {\"role\": \"user\", \"content\": user_prompt}\n",
                "                ],\n",
                "                \"temperature\": 0.7,\n",
                "                \"max_tokens\": 200\n",
                "            }\n",
                "        })\n",
                "    \n",
                "    # Save batch file\n",
                "    batch_file = f\"{output_dir}/batch_a1.jsonl\"\n",
                "    with open(batch_file, 'w') as f:\n",
                "        for request in requests:\n",
                "            f.write(json.dumps(request) + '\\n')\n",
                "    \n",
                "    # Upload and create batch\n",
                "    with open(batch_file, 'rb') as f:\n",
                "        file_response = client.files.create(file=f, purpose=\"batch\")\n",
                "    \n",
                "    batch = client.batches.create(\n",
                "        input_file_id=file_response.id,\n",
                "        endpoint=\"/v1/chat/completions\",\n",
                "        completion_window=\"24h\"\n",
                "    )\n",
                "    \n",
                "    print(f\"Batch created: {batch.id}\")\n",
                "    \n",
                "    # Wait for completion\n",
                "    completed_batch = wait_for_batch(batch.id, \"A1 generation\")\n",
                "    if not completed_batch:\n",
                "        return None\n",
                "    \n",
                "    # Download results\n",
                "    result_file_id = completed_batch.output_file_id\n",
                "    result = client.files.content(result_file_id)\n",
                "    \n",
                "    # Process results\n",
                "    results = {}\n",
                "    for line in result.text.strip().split('\\n'):\n",
                "        response = json.loads(line)\n",
                "        custom_id = response['custom_id']\n",
                "        quiz_content = response['response']['body']['choices'][0]['message']['content']\n",
                "        \n",
                "        results[custom_id] = {\n",
                "            \"item_id\": custom_id,\n",
                "            \"variant\": \"A1\",\n",
                "            \"round\": 1,\n",
                "            \"quiz\": quiz_content\n",
                "        }\n",
                "    \n",
                "    # Save final results\n",
                "    with open(f\"{output_dir}/generated_quiz_A1.json\", 'w') as f:\n",
                "        json.dump(results, f, indent=2)\n",
                "    \n",
                "    print(f\"A1 completed: {len(results)} items\")\n",
                "    return results\n",
                "\n",
                "a1_results = run_a1()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "# A2: Single-agent, reasoning model (150 items)\n",
                "def run_a2():\n",
                "    print(\"Running A2: Single-agent, reasoning model (150 items)\")\n",
                "    \n",
                "    # Create batch requests\n",
                "    requests = []\n",
                "    for i in range(min(150, len(test_data))):\n",
                "        passage = extract_passage(test_data[i]['prompt'])\n",
                "        user_prompt = f\"Here are some examples:\\n\\n{k_shot_examples}\\n\\n---\\n\\nGenerate a quiz for this passage:\\n\\n{passage}\"\n",
                "        \n",
                "        requests.append({\n",
                "            \"custom_id\": f\"A2_{i+1}\",\n",
                "            \"method\": \"POST\",\n",
                "            \"url\": \"/v1/chat/completions\",\n",
                "            \"body\": {\n",
                "                \"model\": \"gpt-5-2025-08-07\",\n",
                "                \"messages\": [\n",
                "                    {\"role\": \"user\", \"content\": f\"{generator_system}\\n\\n{user_prompt}\"}\n",
                "                ]\n",
                "            }\n",
                "        })\n",
                "    \n",
                "    # Save batch file\n",
                "    batch_file = f\"{output_dir}/batch_a2.jsonl\"\n",
                "    with open(batch_file, 'w') as f:\n",
                "        for request in requests:\n",
                "            f.write(json.dumps(request) + '\\n')\n",
                "    \n",
                "    # Upload and create batch\n",
                "    with open(batch_file, 'rb') as f:\n",
                "        file_response = client.files.create(file=f, purpose=\"batch\")\n",
                "    \n",
                "    batch = client.batches.create(\n",
                "        input_file_id=file_response.id,\n",
                "        endpoint=\"/v1/chat/completions\",\n",
                "        completion_window=\"24h\"\n",
                "    )\n",
                "    \n",
                "    print(f\"Batch created: {batch.id}\")\n",
                "    \n",
                "    # Wait for completion\n",
                "    completed_batch = wait_for_batch(batch.id, \"A2 generation\")\n",
                "    if not completed_batch:\n",
                "        return None\n",
                "    \n",
                "    # Download results\n",
                "    result_file_id = completed_batch.output_file_id\n",
                "    result = client.files.content(result_file_id)\n",
                "    \n",
                "    # Process results\n",
                "    results = {}\n",
                "    for line in result.text.strip().split('\\n'):\n",
                "        response = json.loads(line)\n",
                "        custom_id = response['custom_id']\n",
                "        quiz_content = response['response']['body']['choices'][0]['message']['content']\n",
                "        \n",
                "        results[custom_id] = {\n",
                "            \"item_id\": custom_id,\n",
                "            \"variant\": \"A2\",\n",
                "            \"round\": 1,\n",
                "            \"quiz\": quiz_content\n",
                "        }\n",
                "    \n",
                "    # Save final results\n",
                "    with open(f\"{output_dir}/generated_quiz_A2.json\", 'w') as f:\n",
                "        json.dump(results, f, indent=2)\n",
                "    \n",
                "    print(f\"A2 completed: {len(results)} items\")\n",
                "    return results\n",
                "\n",
                "a2_results = run_a2()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "# A3-SS: Standard Generator → Standard Evaluator (950 items, ≤3 rounds)\n",
                "def run_a3_ss():\n",
                "    print(\"Running A3-SS: Standard Generator → Standard Evaluator (950 items)\")\n",
                "    \n",
                "    results = {}\n",
                "    open_items = list(range(min(950, len(test_data))))\n",
                "    \n",
                "    for round_num in range(1, 4):\n",
                "        if not open_items:\n",
                "            break\n",
                "            \n",
                "        print(f\"Round {round_num}: Processing {len(open_items)} items\")\n",
                "        \n",
                "        # Generator batch\n",
                "        gen_requests = []\n",
                "        for i in open_items:\n",
                "            passage = extract_passage(test_data[i]['prompt'])\n",
                "            \n",
                "            if round_num == 1:\n",
                "                user_prompt = f\"Here are some examples:\\n\\n{k_shot_examples}\\n\\n---\\n\\nGenerate a quiz for this passage:\\n\\n{passage}\"\n",
                "            else:\n",
                "                feedback = results.get(f\"A3-SS_{i+1}\", {}).get(\"feedback\", \"Please improve the quiz quality.\")\n",
                "                user_prompt = f\"Here are some examples:\\n\\n{k_shot_examples}\\n\\n---\\n\\nGenerate a quiz for this passage:\\n\\n{passage}\\n\\nPrevious feedback: {feedback}\\nPlease revise accordingly.\"\n",
                "            \n",
                "            gen_requests.append({\n",
                "                \"custom_id\": f\"A3-SS_{i+1}_gen_r{round_num}\",\n",
                "                \"method\": \"POST\",\n",
                "                \"url\": \"/v1/chat/completions\",\n",
                "                \"body\": {\n",
                "                    \"model\": \"gpt-4\",\n",
                "                    \"messages\": [\n",
                "                        {\"role\": \"system\", \"content\": generator_system},\n",
                "                        {\"role\": \"user\", \"content\": user_prompt}\n",
                "                    ],\n",
                "                    \"temperature\": 0.7,\n",
                "                    \"max_tokens\": 200\n",
                "                }\n",
                "            })\n",
                "        \n",
                "        # Run generator batch\n",
                "        gen_batch_file = f\"{output_dir}/batch_a3ss_gen_r{round_num}.jsonl\"\n",
                "        with open(gen_batch_file, 'w') as f:\n",
                "            for request in gen_requests:\n",
                "                f.write(json.dumps(request) + '\\n')\n",
                "        \n",
                "        with open(gen_batch_file, 'rb') as f:\n",
                "            gen_file_response = client.files.create(file=f, purpose=\"batch\")\n",
                "        \n",
                "        gen_batch = client.batches.create(\n",
                "            input_file_id=gen_file_response.id,\n",
                "            endpoint=\"/v1/chat/completions\",\n",
                "            completion_window=\"24h\"\n",
                "        )\n",
                "        \n",
                "        gen_completed = wait_for_batch(gen_batch.id, f\"A3-SS generator round {round_num}\")\n",
                "        if not gen_completed:\n",
                "            break\n",
                "        \n",
                "        # Get generator results\n",
                "        gen_result = client.files.content(gen_completed.output_file_id)\n",
                "        gen_outputs = {}\n",
                "        for line in gen_result.text.strip().split('\\n'):\n",
                "            response = json.loads(line)\n",
                "            custom_id = response['custom_id']\n",
                "            quiz_content = response['response']['body']['choices'][0]['message']['content']\n",
                "            item_id = custom_id.split('_gen_')[0]\n",
                "            gen_outputs[item_id] = quiz_content\n",
                "        \n",
                "        # Evaluator batch\n",
                "        eval_requests = []\n",
                "        for i in open_items:\n",
                "            item_id = f\"A3-SS_{i+1}\"\n",
                "            if item_id not in gen_outputs:\n",
                "                continue\n",
                "                \n",
                "            passage = extract_passage(test_data[i]['prompt'])\n",
                "            quiz_text = gen_outputs[item_id]\n",
                "            eval_prompt = f\"Passage: {passage}\\n\\nGenerated Quiz:\\n{quiz_text}\\n\\nEvaluate this quiz and respond with either \\\"accept\\\" or \\\"revise: [feedback]\\\".\"\n",
                "            \n",
                "            eval_requests.append({\n",
                "                \"custom_id\": f\"{item_id}_eval_r{round_num}\",\n",
                "                \"method\": \"POST\",\n",
                "                \"url\": \"/v1/chat/completions\",\n",
                "                \"body\": {\n",
                "                    \"model\": \"gpt-4\",\n",
                "                    \"messages\": [\n",
                "                        {\"role\": \"system\", \"content\": evaluator_system},\n",
                "                        {\"role\": \"user\", \"content\": eval_prompt}\n",
                "                    ],\n",
                "                    \"temperature\": 0.2,\n",
                "                    \"max_tokens\": 100\n",
                "                }\n",
                "            })\n",
                "        \n",
                "        # Run evaluator batch\n",
                "        eval_batch_file = f\"{output_dir}/batch_a3ss_eval_r{round_num}.jsonl\"\n",
                "        with open(eval_batch_file, 'w') as f:\n",
                "            for request in eval_requests:\n",
                "                f.write(json.dumps(request) + '\\n')\n",
                "        \n",
                "        with open(eval_batch_file, 'rb') as f:\n",
                "            eval_file_response = client.files.create(file=f, purpose=\"batch\")\n",
                "        \n",
                "        eval_batch = client.batches.create(\n",
                "            input_file_id=eval_file_response.id,\n",
                "            endpoint=\"/v1/chat/completions\",\n",
                "            completion_window=\"24h\"\n",
                "        )\n",
                "        \n",
                "        eval_completed = wait_for_batch(eval_batch.id, f\"A3-SS evaluator round {round_num}\")\n",
                "        if not eval_completed:\n",
                "            break\n",
                "        \n",
                "        # Process evaluator results\n",
                "        eval_result = client.files.content(eval_completed.output_file_id)\n",
                "        next_round_items = []\n",
                "        \n",
                "        for line in eval_result.text.strip().split('\\n'):\n",
                "            response = json.loads(line)\n",
                "            custom_id = response['custom_id']\n",
                "            decision = response['response']['body']['choices'][0]['message']['content'].strip().lower()\n",
                "            item_id = custom_id.split('_eval_')[0]\n",
                "            \n",
                "            if decision.startswith(\"accept\") or round_num == 3:\n",
                "                results[item_id] = {\n",
                "                    \"item_id\": item_id,\n",
                "                    \"variant\": \"A3-SS\",\n",
                "                    \"round\": round_num,\n",
                "                    \"quiz\": gen_outputs[item_id]\n",
                "                }\n",
                "            else:\n",
                "                if \"revise:\" in decision:\n",
                "                    feedback = decision.split(\"revise:\", 1)[1].strip()\n",
                "                else:\n",
                "                    feedback = \"Please improve the quiz quality.\"\n",
                "                \n",
                "                results[item_id] = {\"feedback\": feedback}\n",
                "                item_num = int(item_id.split('_')[1]) - 1\n",
                "                next_round_items.append(item_num)\n",
                "        \n",
                "        open_items = next_round_items\n",
                "        print(f\"Round {round_num} complete. {len(next_round_items)} items need revision.\")\n",
                "    \n",
                "    # Clean up feedback-only entries\n",
                "    final_results = {k: v for k, v in results.items() if \"quiz\" in v}\n",
                "    \n",
                "    with open(f\"{output_dir}/generated_quiz_A3-SS.json\", 'w') as f:\n",
                "        json.dump(final_results, f, indent=2)\n",
                "    \n",
                "    print(f\"A3-SS completed: {len(final_results)} items\")\n",
                "    return final_results\n",
                "\n",
                "a3_ss_results = run_a3_ss()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"All scenarios completed!\")\n",
                "print(\"Note: A3-SR, A3-RS, A3-RR scenarios follow similar batch patterns.\")\n",
                "print(\"Implement them following the A3-SS example above.\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python", 
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}

with open('gpt5_completions.ipynb', 'w') as f:
    json.dump(notebook, f, indent=1)

print("Batch API notebook created successfully!")
