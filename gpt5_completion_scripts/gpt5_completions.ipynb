{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in /Users/vh19/.local/share/mise/installs/python/3.13.5/lib/python3.13/site-packages (1.101.0)\n",
      "Requirement already satisfied: python-dotenv in /Users/vh19/.local/share/mise/installs/python/3.13.5/lib/python3.13/site-packages (1.1.1)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/vh19/.local/share/mise/installs/python/3.13.5/lib/python3.13/site-packages (from openai) (4.10.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/vh19/.local/share/mise/installs/python/3.13.5/lib/python3.13/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/vh19/.local/share/mise/installs/python/3.13.5/lib/python3.13/site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /Users/vh19/.local/share/mise/installs/python/3.13.5/lib/python3.13/site-packages (from openai) (0.10.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /Users/vh19/.local/share/mise/installs/python/3.13.5/lib/python3.13/site-packages (from openai) (2.11.7)\n",
      "Requirement already satisfied: sniffio in /Users/vh19/.local/share/mise/installs/python/3.13.5/lib/python3.13/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /Users/vh19/.local/share/mise/installs/python/3.13.5/lib/python3.13/site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /Users/vh19/.local/share/mise/installs/python/3.13.5/lib/python3.13/site-packages (from openai) (4.14.1)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/vh19/.local/share/mise/installs/python/3.13.5/lib/python3.13/site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: certifi in /Users/vh19/.local/share/mise/installs/python/3.13.5/lib/python3.13/site-packages (from httpx<1,>=0.23.0->openai) (2025.8.3)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/vh19/.local/share/mise/installs/python/3.13.5/lib/python3.13/site-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /Users/vh19/.local/share/mise/installs/python/3.13.5/lib/python3.13/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/vh19/.local/share/mise/installs/python/3.13.5/lib/python3.13/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /Users/vh19/.local/share/mise/installs/python/3.13.5/lib/python3.13/site-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /Users/vh19/.local/share/mise/installs/python/3.13.5/lib/python3.13/site-packages (from pydantic<3,>=1.9.0->openai) (0.4.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install openai python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 950 test items\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "client = OpenAI()\n",
    "\n",
    "test_path = os.path.abspath(os.getcwd()).split('gpt5_completion_scripts')[0] + 'processed_data/gpt5/processed_test.jsonl'\n",
    "output_dir = os.path.abspath(os.getcwd()).split('gpt5_completion_scripts')[0] + 'generated_data_gpt5'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "test_data = []\n",
    "with open(test_path, 'r') as f:\n",
    "    for line in f:\n",
    "        test_data.append(json.loads(line))\n",
    "\n",
    "print(f\"Loaded {len(test_data)} test items\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current batches:\n",
      "ID: batch_68a9af26794881909cf83cc02a5e8d2b, Status: completed, Model: unknown\n",
      "ID: batch_68a9ae66123c81909049bacee537926e, Status: completed, Model: unknown\n",
      "ID: batch_68a9ae2e0be08190b0b94df1938451b5, Status: completed, Model: unknown\n",
      "ID: batch_68a9ad067d908190bdae45deee49a144, Status: completed, Model: unknown\n",
      "ID: batch_68a9ac41d25c8190b855e25f04e8c5a3, Status: completed, Model: unknown\n",
      "ID: batch_68a9863304b48190850c54a6a68810c7, Status: completed, Model: unknown\n",
      "ID: batch_68a983d16f348190b7839e1868b5eb51, Status: completed, Model: unknown\n",
      "ID: batch_68a97d6448f08190b59a76459800562d, Status: completed, Model: gpt-4\n",
      "ID: batch_68a9783b1ed881908350fdd57609f3e8, Status: completed, Model: gpt-4\n",
      "ID: batch_68a975ebd06c8190815d559b5037c31c, Status: failed, Model: unknown\n"
     ]
    }
   ],
   "source": [
    "def check_batch_status():\n",
    "    batches = client.batches.list(limit=20)\n",
    "    print(\"Current batches:\")\n",
    "    for batch in batches.data:\n",
    "        metadata = batch.metadata or {}\n",
    "        model = metadata.get('model', 'unknown')\n",
    "        print(f\"ID: {batch.id}, Status: {batch.status}, Model: {model}\")\n",
    "    return batches.data\n",
    "\n",
    "current_batches = check_batch_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete\n",
      "Passage: 108 Wensan Road London, 85 A 100  England\n",
      "March 1st, 2013\n",
      "Dear Lin Tao,\n",
      "I am writing to you in English. I hope you can understand  it.\n",
      "I love studying in London and I have many new friends. Most of them are my classmates. From Monday to Friday, we have English, math, physics and P. E. in the morning. I like English and physics, because they're interesting. I don't like math. It's too boring. At noon, I have to have lunch at school because my home is far from my school. We usually have two classes in the afternoon--art and politics. We finish our classes at 3:30 p. m. After school, my friends and I always play football on the playground. And then we go home by bus.\n",
      "On weekends, we have no classes. We often go to the park and sometimes we go to the movies in the evening. We see movies twice a month. I like some famous  actors like Jackie Chan.\n",
      "Oh, I have no time to write more. Please write back soon.\n",
      "Best wishes,\n",
      "Wang Gang\n",
      ",.\n",
      "\n",
      "Output: Question: Where is Wang Gang studying?\n",
      "True answer: In England.\n",
      "False answer: In America.\n",
      "False answer: In China.\n",
      "False answer: In Canada.\n",
      "###\n",
      "\n",
      "---\n",
      "\n",
      "Passage: My father, at the death of his father, was six years old, and he grew up without education. He moved from Kentucky to Indiana when I was seven. We reached our new home about the time the state came into the Union. It was a wild area, with many bears and other wild animals still in the woods. I grew up there. There were some so-called schools, but what was required of a teacher never went beyond \"reading, writing, and adding.\" If a stranger supposed to understand Latin happened to live for a time in the area, he was looked on as wizard  . There was simply nothing to excite a desire for education. Of course, when I grew up, I did not know much. Still, somehow, I could read, write, and add, but that was all. The advance I have now made is on this store of education, which I have picked up under the pressure of necessity.\n",
      "\n",
      "Output: Question: How did the writer look at his early education?\n",
      "True answer: He thought it was not satisfactory.\n",
      "False answer: He believed he met the school requirements.\n",
      "False answer: He thought he was well-educated.\n",
      "False answer: He believed he was poorly educated.\n",
      "###\n",
      "\n",
      "---\n",
      "\n",
      "Passage: Trans-Bridge Tours is looking forward to providing you with another year of exciting One-Day travel destinations in 2016.\n",
      "Statue of Liberty & Ellis Island\n",
      "August 15 (Sat)\n",
      "$78 Adult; $66 Child (4-12); $ 75 Senior (62+)\n",
      "No visit to the New York City area is complete without a trip to the world's most famous landmarks--- the Statue of Liberty and Ellis Island.\n",
      "Price includes transportation to Liberty State Park and timed ferry tickets.\n",
      "9/11 Museum& Memorial\n",
      "September 29 (Tue)\n",
      "$ 87 Adult; $ 78 Youth (7-17); Senior $80(65+)\n",
      "The Museum displays artifacts associated with the events of 9/11, while presenting stories of loss and recovery. The Memorial between the twin reflecting pools, is a tribute of remembrance and honor to the nearly 3,000 people killed in the terror attacks.\n",
      "The New York Botanical Garden\n",
      "October 14 (Wed)\n",
      "$126 Adult\n",
      "This National Historic Landmark offers visitors 250 acres of the most unique land form of any botanical garden in the world, with huge rock outcroppings, beautiful flower collections, a river, cascading waterfalls, and 40 acres of old-growth forest dating from pre-Revolutionary times plus a world-class Conservatory.\n",
      "Hagley Museum & Winterthur\n",
      "November 21(Sat)\n",
      "$113 Adult\n",
      "Located on 235 acres along the banks of the Brandywine, Hagley is the site of the gunpowder works founded by E.I. du Pont in 1802. Winterthur was the former home of Henry Francis du Pont, a famous antiques collector and gardener.\n",
      "Includes: Brandywine tour at Hagley; Lunch at Hagley Museum; Guided tour at Winterthur Garden\n",
      "\n",
      "Output: Question: Which was set up in memory of the victims of the terrorist attacks?\n",
      "True answer: The 9/11 Memorial.\n",
      "False answer: The Statue of Liberty.\n",
      "False answer: Hagley Museum.\n",
      "False answer: Winterthur Garden.\n",
      "###\n"
     ]
    }
   ],
   "source": [
    "def load_prompt(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        return f.read()\n",
    "\n",
    "generator_system = load_prompt('generator_system_prompt.md')\n",
    "evaluator_system = load_prompt('evaluator_system_prompt.md')\n",
    "\n",
    "def extract_passage(prompt_text):\n",
    "    return prompt_text.split('###')[0].strip()\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "def create_k_shot(data, k=3):\n",
    "    examples = []\n",
    "    k = min(k, len(data))\n",
    "    sampled = random.sample(data, k)\n",
    "    for item in sampled:\n",
    "        passage = extract_passage(item['prompt'])\n",
    "        completion = item['completion'].strip()\n",
    "        examples.append(f\"Passage: {passage}\\n\\nOutput: {completion}\")\n",
    "    return \"\\n\\n---\\n\\n\".join(examples)\n",
    "\n",
    "def wait_for_batch(batch_id, description=\"batch\"):\n",
    "    print(f\"Waiting for {description} to complete...\")\n",
    "    while True:\n",
    "        batch = client.batches.retrieve(batch_id)\n",
    "        print(f\"Status: {batch.status}\")\n",
    "        if batch.status == \"completed\":\n",
    "            return batch\n",
    "        elif batch.status == \"failed\":\n",
    "            print(f\"Batch failed: {batch}\")\n",
    "            return None\n",
    "        time.sleep(30)\n",
    "\n",
    "def process_completed_batch(batch_id, scenario_name, output_filename):\n",
    "    try:\n",
    "        batch = client.batches.retrieve(batch_id)\n",
    "        \n",
    "        if batch.status != \"completed\":\n",
    "            print(f\"Batch {batch_id} status: {batch.status}\")\n",
    "            return None\n",
    "        \n",
    "        result_file_id = batch.output_file_id\n",
    "        result = client.files.content(result_file_id)\n",
    "        \n",
    "        results = {}\n",
    "        for line in result.text.strip().split('\\n'):\n",
    "            response = json.loads(line)\n",
    "            custom_id = response['custom_id']\n",
    "            \n",
    "            if 'error' in response and response['error']:\n",
    "                print(f\"Error in {custom_id}: {response['error']}\")\n",
    "                continue\n",
    "                \n",
    "            if response['response']['status_code'] != 200:\n",
    "                print(f\"API Error in {custom_id}: {response['response']['body']}\")\n",
    "                continue\n",
    "                \n",
    "            quiz_content = response['response']['body']['choices'][0]['message']['content']\n",
    "            \n",
    "            results[custom_id] = {\n",
    "                \"item_id\": custom_id,\n",
    "                \"variant\": scenario_name,\n",
    "                \"round\": 1,\n",
    "                \"quiz\": quiz_content\n",
    "            }\n",
    "        \n",
    "        with open(f\"{output_dir}/{output_filename}\", 'w') as f:\n",
    "            json.dump(results, f, indent=2)\n",
    "        \n",
    "        print(f\"{scenario_name} completed: {len(results)} items saved to {output_filename}\")\n",
    "        return results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing batch {batch_id}: {e}\")\n",
    "        return None\n",
    "        \n",
    "k_shot_examples = create_k_shot(test_data)\n",
    "print(\"Setup complete\")\n",
    "print(k_shot_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_and_transform_gpt5_batch(batch_id, file_name):\n",
    "    batch = client.batches.retrieve(batch_id)\n",
    "    output_file_id = batch.output_file_id\n",
    "    print(output_file_id)\n",
    "    file_response = client.files.content(output_file_id)\n",
    "    results = {}\n",
    "    for line in file_response.text.strip().split('\\n'):\n",
    "        row = json.loads(line)\n",
    "        try:\n",
    "            quiz_text = row[\"response\"][\"body\"][\"output\"][1][\"content\"][0][\"text\"]\n",
    "            index = len(results) + 1\n",
    "            results[str(index)] = quiz_text\n",
    "        except Exception:\n",
    "            continue\n",
    "    with open(os.path.join(os.path.dirname(os.getcwd()), \"generated_data_gpt5\", file_name), \"w\") as f:\n",
    "        json.dump(results, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A1 File Upload - Create and upload batch files\n",
    "def create_a1_batch_files(num_items, batch_suffix=\"\"):\n",
    "    print(f\"Creating A1 batch file ({num_items} items)\")\n",
    "    \n",
    "    requests = []\n",
    "    for i in range(min(num_items, len(test_data))):\n",
    "        passage = extract_passage(test_data[i]['prompt'])\n",
    "        user_prompt = f\"Here are some examples:\\n\\n{k_shot_examples}\\n\\n---\\n\\nGenerate a quiz for this passage:\\n\\n{passage}\"\n",
    "        \n",
    "        requests.append({\n",
    "            \"custom_id\": f\"A1{batch_suffix}_{i+1}\",\n",
    "            \"method\": \"POST\",\n",
    "            \"url\": \"/v1/responses\",\n",
    "            \"body\": {\n",
    "                \"model\": \"gpt-5-2025-08-07\",\n",
    "                \"input\": user_prompt,\n",
    "                \"instructions\": generator_system,\n",
    "                \"reasoning\":{\"effort\": \"minimal\"},\n",
    "                \"max_output_tokens\": 200\n",
    "            }\n",
    "        })\n",
    "    \n",
    "    batch_file = f\"{output_dir}/batch_a1{batch_suffix}.jsonl\"\n",
    "    with open(batch_file, 'w') as f:\n",
    "        for request in requests:\n",
    "            f.write(json.dumps(request) + '\\n')\n",
    "    \n",
    "    print(f\"Estimated tokens: {len(requests) * 200}\")\n",
    "    \n",
    "    # Upload file\n",
    "    with open(batch_file, 'rb') as f:\n",
    "        file_response = client.files.create(file=f, purpose=\"batch\")\n",
    "    \n",
    "    print(f\"File uploaded: {file_response.id}\")\n",
    "    return file_response.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating A1 batch file (5 items)\n",
      "Estimated tokens: 1000\n",
      "File uploaded: file-1BFrhUPT4pA5fBw5vzE8qi\n"
     ]
    }
   ],
   "source": [
    "# Upload A1 test file (5 items)\n",
    "a1_test_file_id = create_a1_batch_files(5, \"_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A1: Single-agent, standard model\n",
    "def run_a1(file_id, batch_suffix=\"\"):\n",
    "    print(f\"Running A1 batch with file: {file_id}\")\n",
    "    \n",
    "    try:\n",
    "        batch = client.batches.create(\n",
    "            input_file_id=file_id,\n",
    "            endpoint=\"/v1/responses\",\n",
    "            completion_window=\"24h\",\n",
    "            metadata={\"scenario\": f\"A1{batch_suffix}\"}\n",
    "        )\n",
    "        \n",
    "        print(f\"A1{batch_suffix} batch created: {batch.id}\")\n",
    "        return batch.id\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running A1 batch with file: file-1BFrhUPT4pA5fBw5vzE8qi\n",
      "A1_test batch created: batch_68a9d24352e88190a7350973d9fa7e63\n"
     ]
    }
   ],
   "source": [
    "# A1 TEST run\n",
    "a1_test_batch = run_a1(a1_test_file_id, \"_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file-LVXb5Pchu7eS32XEazVoh5\n"
     ]
    }
   ],
   "source": [
    "# Download and process the batch file\n",
    "download_and_transform_gpt5_batch(a1_test_batch, \"a1_test.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload A1 full file (950 items) - commented to avoid accidental upload\n",
    "a1_full_file_id = create_a1_batch_files(950)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A1 FULL run\n",
    "a1_full_batch = run_a1(a1_full_file_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_and_transform_gpt5_batch(a1_full_batch, \"a1.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A2 File Upload - Create and upload batch files\n",
    "def create_a2_batch_files(num_items, batch_suffix=\"\"):\n",
    "    print(f\"Creating A2 batch file ({num_items} items)\")\n",
    "    \n",
    "    requests = []\n",
    "    for i in range(min(num_items, len(test_data))):\n",
    "        passage = extract_passage(test_data[i]['prompt'])\n",
    "        user_prompt = f\"Here are some examples:\\n\\n{k_shot_examples}\\n\\n---\\n\\nGenerate a quiz for this passage:\\n\\n{passage}\"\n",
    "        \n",
    "        requests.append({\n",
    "            \"custom_id\": f\"A2{batch_suffix}_{i+1}\",\n",
    "            \"method\": \"POST\",\n",
    "            \"url\": \"/v1/responses\",\n",
    "            \"body\": {\n",
    "                \"model\": \"gpt-5-2025-08-07\",\n",
    "                \"input\": user_prompt,\n",
    "                \"instruction\": generator_system,\n",
    "                \"reasoning\":{\"effort\": \"high\", \"summary\": \"detailed\"},\n",
    "                \"max_output_tokens\": 200\n",
    "            }\n",
    "        })\n",
    "    \n",
    "    batch_file = f\"{output_dir}/batch_a2{batch_suffix}.jsonl\"\n",
    "    with open(batch_file, 'w') as f:\n",
    "        for request in requests:\n",
    "            f.write(json.dumps(request) + '\\n')\n",
    "    \n",
    "    # Upload file\n",
    "    with open(batch_file, 'rb') as f:\n",
    "        file_response = client.files.create(file=f, purpose=\"batch\")\n",
    "    \n",
    "    print(f\"File uploaded: {file_response.id}\")\n",
    "    return file_response.id\n",
    "\n",
    "# Upload A2 test file (3 items)\n",
    "a2_test_file_id = create_a2_batch_files(3, \"_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload A2 full file (150 items) - commented to avoid accidental upload\n",
    "a2_full_file_id = create_a2_batch_files(150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A2: Single-agent, reasoning model\n",
    "def run_a2(file_id, batch_suffix=\"\"):\n",
    "    print(f\"Running A2 batch with file: {file_id}\")\n",
    "    \n",
    "    try:\n",
    "        batch = client.batches.create(\n",
    "            input_file_id=file_id,\n",
    "            endpoint=\"/v1/chat/completions\",\n",
    "            completion_window=\"24h\",\n",
    "            metadata={\"scenario\": f\"A2{batch_suffix}\", \"model\": \"gpt-5-2025-08-07\"}\n",
    "        )\n",
    "        \n",
    "        print(f\"A2{batch_suffix} batch created: {batch.id}\")\n",
    "        return batch.id\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        print(\"Note: GPT-5 model name might be incorrect\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A2 TEST run\n",
    "a2_test_batch = run_a2(a2_test_file_id, \"_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A2 FULL run\n",
    "a2_full_batch = run_a2(a2_full_file_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_and_transform_gpt5_batch(a2_test_batch, \"a2_test.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_and_transform_gpt5_batch(a2_full_batch, \"a2.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process results when batches complete\n",
    "# Example usage:\n",
    "# a1_test_results = process_completed_batch(a1_test_batch, \"A1_test\", \"generated_quiz_A1_test.json\")\n",
    "# a1_full_results = process_completed_batch(\"batch_id_here\", \"A1\", \"generated_quiz_A1.json\")\n",
    "# a2_test_results = process_completed_batch(a2_test_batch, \"A2_test\", \"generated_quiz_A2_test.json\")\n",
    "# a2_full_results = process_completed_batch(\"batch_id_here\", \"A2\", \"generated_quiz_A2.json\")\n",
    "\n",
    "print(\"Separated file upload notebook ready!\")\n",
    "print(\"\")\n",
    "print(\"WORKFLOW:\")\n",
    "print(\"1. File upload cells run once (create + upload batch files)\")\n",
    "print(\"2. Run scenario cells multiple times without re-uploading\")\n",
    "print(\"3. Files are reused, no duplicates created\")\n",
    "print(\"4. Use process_completed_batch() to get results\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
