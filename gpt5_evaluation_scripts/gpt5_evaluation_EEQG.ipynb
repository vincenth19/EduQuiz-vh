{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92e52e15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Downloading datasets-4.0.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting filelock (from datasets)\n",
      "  Downloading filelock-3.19.1-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting numpy>=1.17 (from datasets)\n",
      "  Using cached numpy-2.3.2-cp313-cp313-macosx_14_0_arm64.whl.metadata (62 kB)\n",
      "Collecting pyarrow>=15.0.0 (from datasets)\n",
      "  Downloading pyarrow-21.0.0-cp313-cp313-macosx_12_0_arm64.whl.metadata (3.3 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Using cached dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting pandas (from datasets)\n",
      "  Downloading pandas-2.3.2-cp313-cp313-macosx_11_0_arm64.whl.metadata (91 kB)\n",
      "Collecting requests>=2.32.2 (from datasets)\n",
      "  Downloading requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting tqdm>=4.66.3 (from datasets)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.5.0-cp313-cp313-macosx_11_0_arm64.whl.metadata (12 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py312-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec<=2025.3.0,>=2023.1.0 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting huggingface-hub>=0.24.0 (from datasets)\n",
      "  Downloading huggingface_hub-0.34.4-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: packaging in /Users/vh19/.local/lib/python3.13/site-packages (from datasets) (25.0)\n",
      "Collecting pyyaml>=5.1 (from datasets)\n",
      "  Using cached PyYAML-6.0.2-cp313-cp313-macosx_11_0_arm64.whl.metadata (2.1 kB)\n",
      "Collecting aiohttp!=4.0.0a0,!=4.0.0a1 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading aiohttp-3.12.15-cp313-cp313-macosx_11_0_arm64.whl.metadata (7.7 kB)\n",
      "Collecting typing-extensions>=3.7.4.3 (from huggingface-hub>=0.24.0->datasets)\n",
      "  Using cached typing_extensions-4.14.1-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting hf-xet<2.0.0,>=1.1.3 (from huggingface-hub>=0.24.0->datasets)\n",
      "  Downloading hf_xet-1.1.8-cp37-abi3-macosx_11_0_arm64.whl.metadata (703 bytes)\n",
      "Collecting charset_normalizer<4,>=2 (from requests>=2.32.2->datasets)\n",
      "  Downloading charset_normalizer-3.4.3-cp313-cp313-macosx_10_13_universal2.whl.metadata (36 kB)\n",
      "Collecting idna<4,>=2.5 (from requests>=2.32.2->datasets)\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests>=2.32.2->datasets)\n",
      "  Using cached urllib3-2.5.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests>=2.32.2->datasets)\n",
      "  Using cached certifi-2025.8.3-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/vh19/.local/lib/python3.13/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas->datasets)\n",
      "  Using cached pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas->datasets)\n",
      "  Using cached tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.4.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Using cached aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Using cached attrs-25.3.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading frozenlist-1.7.0-cp313-cp313-macosx_11_0_arm64.whl.metadata (18 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading multidict-6.6.4-cp313-cp313-macosx_11_0_arm64.whl.metadata (5.3 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading propcache-0.3.2-cp313-cp313-macosx_11_0_arm64.whl.metadata (12 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading yarl-1.20.1-cp313-cp313-macosx_11_0_arm64.whl.metadata (73 kB)\n",
      "Requirement already satisfied: six>=1.5 in /Users/vh19/.local/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Downloading datasets-4.0.0-py3-none-any.whl (494 kB)\n",
      "Using cached dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Downloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
      "Downloading huggingface_hub-0.34.4-py3-none-any.whl (561 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m561.5/561.5 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multiprocess-0.70.16-py312-none-any.whl (146 kB)\n",
      "Using cached numpy-2.3.2-cp313-cp313-macosx_14_0_arm64.whl (5.1 MB)\n",
      "Downloading pyarrow-21.0.0-cp313-cp313-macosx_12_0_arm64.whl (31.2 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.2/31.2 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached PyYAML-6.0.2-cp313-cp313-macosx_11_0_arm64.whl (171 kB)\n",
      "Downloading requests-2.32.5-py3-none-any.whl (64 kB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Downloading filelock-3.19.1-py3-none-any.whl (15 kB)\n",
      "Downloading pandas-2.3.2-cp313-cp313-macosx_11_0_arm64.whl (10.7 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.7/10.7 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m01\u001b[0m\n",
      "\u001b[?25hDownloading xxhash-3.5.0-cp313-cp313-macosx_11_0_arm64.whl (30 kB)\n",
      "Downloading aiohttp-3.12.15-cp313-cp313-macosx_11_0_arm64.whl (466 kB)\n",
      "Using cached certifi-2025.8.3-py3-none-any.whl (161 kB)\n",
      "Downloading charset_normalizer-3.4.3-cp313-cp313-macosx_10_13_universal2.whl (205 kB)\n",
      "Downloading hf_xet-1.1.8-cp37-abi3-macosx_11_0_arm64.whl (2.6 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Using cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Using cached typing_extensions-4.14.1-py3-none-any.whl (43 kB)\n",
      "Using cached tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Using cached urllib3-2.5.0-py3-none-any.whl (129 kB)\n",
      "Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Using cached aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
      "Using cached attrs-25.3.0-py3-none-any.whl (63 kB)\n",
      "Downloading frozenlist-1.7.0-cp313-cp313-macosx_11_0_arm64.whl (45 kB)\n",
      "Downloading multidict-6.6.4-cp313-cp313-macosx_11_0_arm64.whl (43 kB)\n",
      "Downloading propcache-0.3.2-cp313-cp313-macosx_11_0_arm64.whl (41 kB)\n",
      "Downloading yarl-1.20.1-cp313-cp313-macosx_11_0_arm64.whl (88 kB)\n",
      "Installing collected packages: pytz, xxhash, urllib3, tzdata, typing-extensions, tqdm, pyyaml, pyarrow, propcache, numpy, multidict, idna, hf-xet, fsspec, frozenlist, filelock, dill, charset_normalizer, certifi, attrs, aiohappyeyeballs, yarl, requests, pandas, multiprocess, aiosignal, huggingface-hub, aiohttp, datasets\n",
      "Successfully installed aiohappyeyeballs-2.6.1 aiohttp-3.12.15 aiosignal-1.4.0 attrs-25.3.0 certifi-2025.8.3 charset_normalizer-3.4.3 datasets-4.0.0 dill-0.3.8 filelock-3.19.1 frozenlist-1.7.0 fsspec-2025.3.0 hf-xet-1.1.8 huggingface-hub-0.34.4 idna-3.10 multidict-6.6.4 multiprocess-0.70.16 numpy-2.3.2 pandas-2.3.2 propcache-0.3.2 pyarrow-21.0.0 pytz-2025.2 pyyaml-6.0.2 requests-2.32.5 tqdm-4.67.1 typing-extensions-4.14.1 tzdata-2025.2 urllib3-2.5.0 xxhash-3.5.0 yarl-1.20.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ced4de7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vh19/.local/share/mise/installs/python/3.13.5/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "import json\n",
    "import os\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "76867e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change these variables to perform evalution for you task\n",
    "original_quiz_path = os.path.abspath(os.getcwd()).split('gpt5_evaluation_scripts')[0] + '/processed_data/gpt3/completion_4/processed_test.json'\n",
    "generated_quiz_path = os.path.abspath(os.getcwd()).split('gpt5_evaluation_scripts')[0] + '/generated_data_gpt5/EEQG/generated_quiz.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "70b66793",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/vh19/Documents/Works/personal-projects/EduQuiz-vh//generated_data_gpt5/EEQG/generated_quiz.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Get generated quizzes\u001b[39;00m\n\u001b[32m      8\u001b[39m generated_quizzes = []\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mgenerated_quiz_path\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m     10\u001b[39m     generated_quizzes_dict = json.load(f)\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m generated_quizzes_dict:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.13/site-packages/IPython/core/interactiveshell.py:343\u001b[39m, in \u001b[36m_modified_open\u001b[39m\u001b[34m(file, *args, **kwargs)\u001b[39m\n\u001b[32m    336\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m}:\n\u001b[32m    337\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    338\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIPython won\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m by default \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    339\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    340\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33myou can use builtins\u001b[39m\u001b[33m'\u001b[39m\u001b[33m open.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    341\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m343\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '/Users/vh19/Documents/Works/personal-projects/EduQuiz-vh//generated_data_gpt5/EEQG/generated_quiz.json'"
     ]
    }
   ],
   "source": [
    "# Get original quizzes\n",
    "original_quizzes = []\n",
    "\n",
    "for line in open(original_quiz_path):\n",
    "    original_quizzes.append(json.loads(line)['completion'].split(\"\\n###\")[0].replace(\"\\n\",\" \").strip())\n",
    "    \n",
    "# Get generated quizzes\n",
    "generated_quizzes = []\n",
    "with open(generated_quiz_path) as f:\n",
    "    generated_quizzes_dict = json.load(f)\n",
    "\n",
    "for key in generated_quizzes_dict:\n",
    "    generated_quizzes.append(generated_quizzes_dict[key].replace(\"\\n\",\" \").strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d9dfcfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put the predictions and gold references in lists\n",
    "\n",
    "predictions = []\n",
    "gold_references = []\n",
    "predictions_list = []\n",
    "gold_references_list = []\n",
    "\n",
    "for i in range(len(original_quizzes)):\n",
    "    predictions.append(generated_quizzes[i])\n",
    "    gold_references.append(original_quizzes[i])\n",
    "\n",
    "    predictions_list.append(generated_quizzes[i].split(' '))\n",
    "    gold_references_list.append([original_quizzes[i].split(' ')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7514ed0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:228: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject\n"
     ]
    }
   ],
   "source": [
    "bleu = datasets.load_metric('bleu')\n",
    "rouge = datasets.load_metric('rouge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "957df6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "bleu.add_batch(predictions=predictions_list, references=gold_references_list)\n",
    "rouge.add_batch(predictions=predictions, references=gold_references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ccc14283",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_bleu = bleu.compute()\n",
    "final_rouge = rouge.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ddd7208e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define path of downloaded meteor from https://www.cs.cmu.edu/~alavie/METEOR/\n",
    "meteor_path = \"<PATH_TO_YOUR_DOWNLOADED_METEOR>\"\n",
    "\n",
    "# Move results to meteor directory\n",
    "with open(meteor_path + \"predictions.txt\", 'w') as f:\n",
    "    for i in range(len(predictions)):\n",
    "        f.write(str(predictions[i]) + '\\n')\n",
    "        \n",
    "with open(meteor_path + \"ground_truth.txt\", 'w') as f:\n",
    "    for i in range(len(gold_references)):\n",
    "        f.write(str(gold_references[i]) + '\\n')\n",
    "\n",
    "# Run the meteor command from the meteor directory and remove result files again   \n",
    "wd = os.getcwd()\n",
    "os.chdir(meteor_path)\n",
    "output = os.popen(\"java -Xmx2G -jar meteor-*.jar predictions.txt ground_truth.txt -l en -norm\").read()\n",
    "os.remove(meteor_path + \"predictions.txt\")\n",
    "os.remove(meteor_path + \"ground_truth.txt\")\n",
    "os.chdir(wd)\n",
    "\n",
    "# Get the score from the output\n",
    "meteor_score = round(float(output.split(\"Final score:\")[1].strip()) * 100, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "54f4a662",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU:  11.61\n",
      "ROUGE-L:  36.11\n",
      "METEOR:  25.42\n"
     ]
    }
   ],
   "source": [
    "print(\"BLEU: \", str(round(final_bleu['bleu'] * 100, 2)))\n",
    "print(\"ROUGE-L: \", str(round(final_rouge['rougeL'].mid.fmeasure * 100, 2)))\n",
    "print(\"METEOR: \", str(meteor_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "331f30af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create result files\n",
    "        \n",
    "with open(os.path.abspath(os.getcwd()).split('gpt3_evaluation_scripts')[0] + 'generated_data_gpt3/EEQG/' + 'automatic_evaluation_EEQG.txt', 'w') as f:\n",
    "    f.write(\"BLEU: \" + str(round(final_bleu['bleu'] * 100, 2)))\n",
    "    f.write('\\n')\n",
    "    f.write(\"ROUGE-L: \" + str(round(final_rouge['rougeL'].mid.fmeasure * 100, 2)))\n",
    "    f.write('\\n')\n",
    "    f.write(\"METEOR: \" + str(meteor_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eacbe38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all the target prompts and completions\n",
    "test_data = []\n",
    "\n",
    "for line in open(original_quiz_path):\n",
    "    test_data.append((json.loads(line)))\n",
    "\n",
    "# Get quizzes\n",
    "quizzes = []\n",
    "with open(generated_quiz_path) as f:\n",
    "    gen_quizzes = json.load(f)\n",
    "\n",
    "for key in gen_quizzes:\n",
    "    quizzes.append(gen_quizzes[key])\n",
    "\n",
    "count = 0\n",
    "with open(os.path.abspath(os.getcwd()).split('gpt3_evaluation_scripts')[0] + 'human_evaluation/EEQG/' + 'EEQG_gpt3.txt', 'w') as f:\n",
    "    for i in range(0,len(test_data),math.floor(len(test_data)/100)):\n",
    "        f.write('Excel row: ' + str(count+2) + ' Test instance: ' + str(i+1) + '\\n\\n')\n",
    "        f.write(test_data[i]['prompt'].split(\"\\n\\n###\")[0] + '\\n\\n')\n",
    "        f.write('Generated quiz:\\n')\n",
    "        f.write(quizzes[i].strip() + '\\n\\n')\n",
    "        f.write('----------------------------------------------------------------------------------------' + '\\n\\n')\n",
    "        count+=1\n",
    "        \n",
    "        if count == 100:\n",
    "            break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
